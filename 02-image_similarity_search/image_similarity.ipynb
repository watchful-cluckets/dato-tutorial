{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "from IPython.display import Image\n",
    "## Set a variable for the path to your (shared) data and/or model directory\n",
    "path_to_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Simiarlity Service\n",
    "----------\n",
    "\n",
    "Who doesn't love dresses? Dresses can be incredibly diverse in terms of look, fit, feel, material, trendiness, and quality. They help us feel good, look good, and express ourselves. Needless to say, buying and selling dresses is a big deal. Depending on who you are, what you need, what you want, and what you can afford, this can be really hard. Sometimes it's just hard because you like everything and need to make a decision.\n",
    "\n",
    "In the keynote demo, we saw an end-to-end application that finds similar items based on text and image metadata. We're going to focus on one of the core parts of building such an application, Image Similarity, that uses transfer learning and nearest neighbours to extract and compare features on images and determine how similar they are.\n",
    "\n",
    "\n",
    "This iPython notebook consists of the following parts:\n",
    "1. Loading the Data - loading existing data into an SFrame.\n",
    "2. Extracing the features - loading an existing pre-trained model into a model object and using it to extract features.\n",
    "3. Calculating the Distance - Creating and training a k-nearest neighbors classification model on the extracted features.\n",
    "4. Finding Similar Items - Using the k-nearest neighbors model to help us find similar items.\n",
    "5. Saving our new model for use in deploying a predictive service! (This is covered in a separate notebook)\n",
    "\n",
    "<img src='images/workflow1.png'></img>\n",
    "\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "Make sure you've followed the handout and have Graphlab-Create installed, along with the data/model needed for this tutorial. Please let a volunteer know if you have any trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 1: Load the Data\n",
    "----------\n",
    "In this first part, we will introduce two things:\n",
    "1. A rich product dataset including images and product metadata (data from the keynote demo!).\n",
    "2. A deep neural network model trained on over 1.2 million images from the ImageNet 2012 competition.\n",
    "\n",
    "\n",
    "### Load images from product dataset\n",
    "This dataset includes product metadata (descriptions, category information, price, brand, and image features), links, and some features engineered from other columns (bag of words, TFIDF, etc.). We're loading this data into an SFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating the SFrame with our path to the directory where it is saved.\n",
    "image_sf = gl.SFrame(path_to_dir + \n",
    "                     'sf_processed.sframe'\n",
    "                    )\n",
    "\n",
    "image_sf\n",
    "#image_sf.show()  #Explore the data using Canvas visual explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load an existing deep learning model\n",
    "Here we load the model that will be used to extract visual features extracted via a trained deep neural network. This is an AlexNet architecture that has been trained on the ImageNet data set for 45 iterations.\n",
    "\n",
    "<img src='images/AlexNet.png'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretrained_model = gl.load_model(path_to_dir + \n",
    "                                 'imagenet_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, when you implement your own transfer-learning solutions, you will have to train your model on a dataset that fits your problem space, whether that's ImageNet data from a given year or something else. You'll benefit most from the generality of the features you extract from your trained model if the data is comprehensive. You can learn how to build the ImageNet model we used for this application on the Dato website <a href='https://dato.com/learn/gallery/notebooks/build_imagenet_deeplearning.html'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's take a look at the network topology of hte pretrained model\n",
    "pretrained_model['network']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 2: Extract Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/workflow2.png'></img>\n",
    "\n",
    "\n",
    "We will be using deep visual features to match the product images to each other. In order to do that, we need to load in our pre-trained ImageNet neural network model to be used as a feature extractor, and extract features from the images in the dataset. \n",
    "\n",
    "<img src='images/feature_extraction.png'></img>\n",
    "\n",
    "To learn more about feature extraction, read this [blog post](http://blog.dato.com/deep-learning-blog-post)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example extract the features of the first image in the dataset. The <code>extract_features</code> takes the output from the second last layer of the pretrailed_model, before the classification layer discards the rich features the network has learned.\n",
    "\n",
    "We also observe the size of the features extracted, compared with the original image:\n",
    "\n",
    "- 196608: bytes values per 256x256pixel image with an RGB component.\n",
    "- 4096: size of the penultimate fully-connected layer and hence the # of features 4096."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_sf['image'][:10].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here are the features of the first image\n",
    "extracted = pretrained_model.extract_features(image_sf[['image']][:1])\n",
    "# NB: image_sf is an SFrame, image_sf['image'] is an SArray, and image_sf[['image']] is an SFrame\n",
    "extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the features for all the images in our dataset using `extract_features`. This does the following:\n",
    "\n",
    "1. For each image in our dataset, we progagate it through our pre-trained neural network.\n",
    "2. At each layer of the neural network, some or all neurons are excited -- to various degrees -- by our image.\n",
    "3. We can represent the excitement of every neuron in a given layer as a vector of all of their excitements.\n",
    "4. There's an additional parameter `layer_id`, that allows you to choose any fully-connected layer to extract features from. The default is `layer_id=None`, which returns features from the penultimate layer of the pre-trained DNN of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_features = pretrained_model.extract_features(image_sf[['image']]) #Caution! this may take a while\n",
    "image_sf['features'] = extracted_features ## adding the extracted_features to our SFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculating the Distance\n",
    "<img src='images/workflow3.png'></img>\n",
    "\n",
    "This is the last step in building the similar items recommdendation model. Using the features we extracted above, we are going to create a *k*-Nearest Neighbors model that measures the distance between all our features enables end users to find products whose images match most closely.\n",
    "\n",
    "<img src='images/nearest_neighbors.png'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_model = gl.nearest_neighbors.create(image_sf, \n",
    "                                       features=['features'])  # We're using the pre-extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Find Similar Items\n",
    "<img src='images/workflow4.png'></img>\n",
    "\n",
    "### Query 1: Blue, da ba dee da ba die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blue = image_sf[194:195]\n",
    "blue['image'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "integer = 42 ##number of nearest neighbors you want to query, can be in the range(1, len(image_sf))\n",
    "\n",
    "similar_to_blue = nn_model.query(blue, \n",
    "                                 k=integer, \n",
    "                                 verbose=True)\n",
    "\n",
    "similar_to_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## To get the images, we need to join the reference label in our kNN model to our main SFrame\n",
    "blue_images = image_sf.join(similar_to_blue, \n",
    "                            on={'_id':'reference_label'}\n",
    "                           ).sort('distance')\n",
    "\n",
    "## Let's show just the first 10 nearest neighbors\n",
    "blue_images['image'][0:10].show() ## notice how we're taking a length 10 slice of the query we did above, of size 42."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Similarity with more unique images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interesting = image_sf[2:3]\n",
    "interesting['image'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similar_to_interesting = nn_model.query(blue, k=10, verbose=True)\n",
    "similar_to_interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## To get the images, we need to join the reference label in our kNN model to our main SFrame\n",
    "interesting_images = image_sf.join(similar_to_interesting, \n",
    "                                   on={'_id':'reference_label'}\n",
    "                                  ).sort('distance')\n",
    "\n",
    "## Let's show just the first 10 nearest neighbors\n",
    "interesting_images['image'][0:10].show() ## notice how we're taking a length 10 slice of the query we did above, of size 42."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for deploying a predictive service\n",
    "\n",
    "Now we are going to save the nearest neighbors model we've created and trained, in order to create, run, and deploy a predictive service that you can interact with via a RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_model.save('nearest_dress_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now that we've shown how to take an image, query the kNN model for some number of that image's nearest neigbors, and then use our original dataset to find the images for those neighbors, we want you to try to find something new.\n",
    "\n",
    "### Exercise 1: Least similar to Blue\n",
    "\n",
    "Find the dress that is LEAST similar to the dress we found above, which, for the record, is:\n",
    "\n",
    "```python\n",
    "blue = image_sf[194:195]\n",
    "blue['image'].show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: What is the most average dress?\n",
    "\n",
    "Find the dress that is the most AVERAGE across all of the dresses, in terms of its extracted visual features.\n",
    "\n",
    "Remember that we stored these visual features in our SFrame `image_sf` in the `extracted_features` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: What is the most unique dress?\n",
    "\n",
    "Find the dress that is the most unique across all of the dresses, in terms of its extracted visual features.\n",
    "\n",
    "Remember that we stored these visual features in our SFrame `image_sf` in the `extracted_features` column.\n",
    "\n",
    "**HINT:** which dress is, in aggregate, farthest from every other dress?\n",
    "\n",
    "**NOTE:** In Exercise 2, we found the most average dress. You may be thinking, \"wouldn't the most unique dress be the dress whose features are furthest from the average?\" For some datasets, this may be true, but in general, it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solutions\n",
    "\n",
    "### Solution to Exercise 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blue = image_sf[194:195] ## Here's our image\n",
    "blue_neighbors = nn_model.query(blue, \n",
    "                                k=len(image_sf), \n",
    "                                verbose=True) ## Query the kNN model for all neighbors\n",
    "\n",
    "blue_neighbor_images = image_sf.join(blue_neighbors, \n",
    "                                     on={'_id':'reference_label'}\n",
    "                                    ).sort('distance') ## Do the join\n",
    "\n",
    "blue_neighbor_images['image'][-1:].show() ## The least similar image is the one whose distance is greatest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Exercise 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_features = image_sf['features'].mean() # get the mean vector\n",
    "\n",
    "# Add a column, with all values as mean vector\n",
    "image_sf['mean_features'] = image_sf.apply(lambda x: mean_features)\n",
    "\n",
    "# Center all features on the mean features vector\n",
    "image_sf['centered_features'] = image_sf['features'] - image_sf['mean_features']\n",
    "\n",
    "# Find magnitude of all centered vectors (i.e., Euclidean distance from the average feature vector)\n",
    "from numpy import linalg ## good linear algebra library\n",
    "image_sf['from_center'] = image_sf['centered_features'].apply(lambda x: linalg.norm(x)) ## norm is Euclidean\n",
    "\n",
    "# Sort the SFrame by smallest to largest\n",
    "# The centered_features vector with the smallest magnitude is  visually the most \"average\" dress in the dataset.\n",
    "image_sf.sort('from_center')[0:1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Exercise 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create SFrame of all nearest neighbors for all dresses\n",
    "## This could take a few minutes (consider using Dato Distributed)\n",
    "all_nn = nn_model.query(image_sf, \n",
    "                        k=len(image_sf), \n",
    "                        verbose=True)\n",
    "\n",
    "## Now, for each dress, take the sum of all distances from it to all other dresses\n",
    "total_distances = all_nn.groupby(key_columns='query_label', \n",
    "                                 operations = {'sum' : gl.aggregate.SUM('distance')})\n",
    "\n",
    "## Find the label for the most unique dress, i.e., the dress with the highest sum of inter-dress distances\n",
    "unique = total_distances.sort('sum')[-1]['query_label']\n",
    "\n",
    "## Let's see what we have\n",
    "image_sf[unique:unique+1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_sf[unique:unique+1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "----------\n",
    "\n",
    "Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. arXiv:1409.0575, 2014.\n",
    "\n",
    "Donahue, J., Jia, Y., Vinyals, O., Homan, J., Zhang, N., Tzeng, E., and Darrell, T. DeCAF: A deep convolutional activation feature for generic visual recognition. In JMLR, 2014.\n",
    "\n",
    "Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" In Advances in neural information processing systems, pp. 1097-1105. 2012.\n",
    "\n",
    "Yang, J., L., Y., Tian, Y., Duan, L., and Gao, W. Group-sensitive multiple kernel learning for object categorization. In ICCV, 2009.\n",
    "\n",
    "Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. \"Intriguing properties of neural networks.\" arXiv preprint arXiv:1312.6199 (2013).\n",
    "\n",
    "Nguyen, Anh, Jason Yosinski, and Jeff Clune. \"Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.\" arXiv preprint arXiv:1412.1897 (2014).\n",
    "\n",
    "Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" arXiv preprint arXiv:1412.6572 (2014)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
